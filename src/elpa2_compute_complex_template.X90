#if 0
!    This file is part of ELPA.
!
!    The ELPA library was originally created by the ELPA consortium,
!    consisting of the following organizations:
!
!    - Max Planck Computing and Data Facility (MPCDF), fomerly known as
!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
!      Informatik,
!    - Technische Universität München, Lehrstuhl für Informatik mit
!      Schwerpunkt Wissenschaftliches Rechnen ,
!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
!      and
!    - IBM Deutschland GmbH
!
!    This particular source code file contains additions, changes and
!    enhancements authored by Intel Corporation which is not part of
!    the ELPA consortium.
!
!    More information can be found here:
!    http://elpa.mpcdf.mpg.de/
!
!    ELPA is free software: you can redistribute it and/or modify
!    it under the terms of the version 3 of the license of the
!    GNU Lesser General Public License as published by the Free
!    Software Foundation.
!
!    ELPA is distributed in the hope that it will be useful,
!    but WITHOUT ANY WARRANTY; without even the implied warranty of
!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
!    GNU Lesser General Public License for more details.
!
!    You should have received a copy of the GNU Lesser General Public License
!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
!
!    ELPA reflects a substantial effort on the part of the original
!    ELPA consortium, and we ask you to respect the spirit of the
!    license that we chose: i.e., please contribute any changes you
!    may have back to the original ELPA library distribution, and keep
!    any derivatives of ELPA under the same license that we chose for
!    the original distribution, the GNU Lesser General Public License.
!
!
! ELPA1 -- Faster replacements for ScaLAPACK symmetric eigenvalue routines
!
! Copyright of the original code rests with the authors inside the ELPA
! consortium. The copyright of any additional modifications shall rest
! with their original authors, but shall adhere to the licensing terms
! distributed along with the original code in the file "COPYING".



! ELPA2 -- 2-stage solver for ELPA
!
! Copyright of the original code rests with the authors inside the ELPA
! consortium. The copyright of any additional modifications shall rest
! with their original authors, but shall adhere to the licensing terms
! distributed along with the original code in the file "COPYING".
#endif

#define COMPLEXCASE 1
#undef REALCASE
#include "elpa2_bandred_template.X90"
#undef COMPLEXCASE
#include "elpa2_herm_matrix_allreduce_complex_template.X90"
#define COMPLEXCASE 1
#include "elpa2_trans_ev_band_to_full_template.X90"
#include "elpa2_tridiag_band_template.X90"
#include "elpa2_trans_ev_tridi_to_band_template.X90"

    subroutine compute_hh_dot_products_complex_gpu_PRECISION(nbw, n)
      use cuda_c_kernel
      use precision
      implicit none
      integer(kind=ik), value :: nbw, n

      if (n .le. 1) return
      call launch_compute_hh_dotp_c_kernel_complex_PRECISION( bcast_buffer_dev, hh_dot_dev, nbw,n)
     end subroutine

     subroutine pack_row_group_complex_gpu_PRECISION(rows, n_offset, row_count)
       use cuda_c_kernel
       use precision
       implicit none
       integer(kind=ik), intent(in) :: n_offset, row_count
       complex(kind=COMPLEX_DATATYPE)             :: rows(:,:)
       integer(kind=ik)             :: max_idx
       logical                      :: successCUDA

       max_idx = (stripe_count - 1) * stripe_width + last_stripe_width
       call launch_my_pack_c_kernel_complex_PRECISION(row_count, n_offset, max_idx, stripe_width,a_dim2, stripe_count, &
                                            l_nev, aIntern_dev, row_group_dev)
       successCUDA =  cuda_memcpy( loc(rows(:, 1: row_count)), row_group_dev ,row_count * l_nev * size_of_PRECISION_complex, &
                                  cudaMemcpyDeviceToHost)
       if (.not.(successCUDA)) then
         print *,"pack_row_group_complex_gpu: error in cudaMemcpy"
         stop
       endif

     end subroutine

     subroutine unpack_row_group_complex_gpu_PRECISION(rows, n_offset, row_count)
       use cuda_c_kernel
       use precision
       implicit none
       integer(kind=ik), intent(in)    :: n_offset, row_count
       complex(kind=COMPLEX_DATATYPE), intent(in)    :: rows(:, :)
       integer(kind=ik)                :: max_idx
       integer(kind=ik)                :: i
       logical                         :: successCUDA

       max_idx = (stripe_count - 1) * stripe_width + last_stripe_width
       successCUDA =  cuda_memcpy( row_group_dev , loc(rows(1, 1)),row_count * l_nev* size_of_PRECISION_complex , &
                                  cudaMemcpyHostToDevice)
       if (.not.(successCUDA)) then
         print *,"unpack_row_group_complex_gpu: error in cudaMemcpy"
         stop
       endif
       call launch_my_unpack_c_kernel_complex_PRECISION( row_count, n_offset,max_idx,stripe_width,a_dim2, stripe_count, l_nev, &
                                              row_group_dev,aIntern_dev)
     end subroutine

     subroutine unpack_and_prepare_row_group_complex_gpu_PRECISION(next_unpack_idx, force)

       use precision
       implicit none
       integer(kind=ik), intent(in) :: next_unpack_idx
       logical, intent(in)          :: force

       if (row_group_size == 0) then
         ! Nothing to flush, just prepare for the upcoming row
         row_group_size = 1
       else
         if (force .or. (row_group_size == nblk) .or. (unpack_idx + 1 /=next_unpack_idx)) then
           ! A flush and a reset must  performed
           call unpack_row_group_complex_gpu_PRECISION(row_group(:, :), unpack_idx - row_group_size, row_group_size)
           row_group_size = 1
         else
           ! Just prepare for the upcoming row
           row_group_size = row_group_size + 1
         endif
       endif
       ! Always update the index for the upcoming row
       unpack_idx = next_unpack_idx

    end subroutine

    subroutine compute_hh_trafo_complex_gpu_PRECISION(off, ncols, istripe, a_off, dev_offset, dev_offset_1, dev_offset_2)

      use iso_c_binding
      use cuda_c_kernel
      use precision
      implicit none
      integer(kind=ik), intent(in) :: off, ncols, istripe
      integer(kind=ik)             :: nl
      real(kind=c_double)          :: ttt ! MPI_WTIME always needs double

      integer(kind=ik)             :: a_off
      integer(kind=c_size_t)       :: dev_offset, dev_offset_1, dev_offset_2

      if (ncols < 1) return
      ttt = mpi_wtime()
      nl = merge(stripe_width, last_stripe_width, istripe < stripe_count)

      dev_offset = (0 + ( (  a_off + off-1 )* stripe_width) + ( (istripe - 1)*stripe_width*a_dim2 )) * &
                    size_of_PRECISION_complex
      dev_offset_1 = (0 +  (  off-1 )* nbw) *size_of_PRECISION_complex
      dev_offset_2 =( off-1 )*size_of_PRECISION_complex

!      t1_compute_kernel =MPI_Wtime()
      call launch_compute_hh_trafo_c_kernel_complex_PRECISION(aIntern_dev + dev_offset,bcast_buffer_dev + dev_offset_1, &
                                                    hh_tau_dev + dev_offset_2, nl, nbw,stripe_width, off,ncols)

!      time0 = time0 + time1
!      t2_compute_kernel =MPI_Wtime()
!      t0_compute_kernel =  t0_compute_kernel + t2_compute_kernel-t1_compute_kernel

      kernel_flops = kernel_flops + 4 * int(nl, 8) * int(ncols, 8) * int(nbw,8)
      kernel_time = kernel_time + mpi_wtime() - ttt
      n_times =n_times +1
    end subroutine
end subroutine

